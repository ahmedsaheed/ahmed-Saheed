---
title: "Asymptotic Analysis"
description: "growth of a runtime/rate of growth of an algorithm without worrying about different computers , compilers or implementation."
date: "2022-06-20"
---

## Intro to Asymptotic Analysis

Because the time taken to perform an algorithms differs due to input size, compilers, computer language & hardware, they are not efficiently calculated using methods like benchmarking or mere assumptions. **This is where asymptotic analysis comes in:**

## Asymptotic Analysis

During asymptotic analysis we do not really calculate time as said earlier, instead we calculate the amount of operations in the [[algorithm]] because we expect performance “time” to be dependant on both **size of input** and machine power. ([Refer to khan material for better explanation](https://www.khanacademy.org/computing/computer-science/algorithms/asymptotic-notation/a/asymptotic-notation)).

Formal Definition: It is a way of describing **growth of a runtime/rate of growth** of an algorithm without worrying about different computers , compilers or implementation.

## Asymptotic Notations

1.  BigO notation ⇒ Upper Bound
2.  Omega ⇒ Lower Bound
3.  Theta ⇒ Both upper and lower bound also called tight bound.

## Further Reading

-   [High Level Illustration can be found here](/fun)
-   [Big0 Cheat Sheet](https://www.bigocheatsheet.com/)
-   [MIT Asymptotic Analysis Recitation 2011](http://courses.csail.mit.edu/6.006/spring11/rec/rec01.pdf)
-   [More consise explanation of time complexities](<https://medium.com/swlh/big-o-notation-and-time-space-complexity-1806936e6330#:~:text=O(nlogn)%20is%20known%20as,using%20O(logn)%20space.>)
